---
title: "Data_exploration_assignment"
author: "WanchiPeng"
date: '2022-07-24'
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries: loading all the libraries

```{r}
library(tidyverse)
library(lubridate)
library(ggplot2)
library(ggforce)
library(zoom)
library(fixest)
library(vtable)
```

# Use list.files(), read_csv and bind_rows() to read and combine as data frame.

```{r}
list_filenames <- list.files(path="../data/", pattern="trends_up_to_", all.files=FALSE, full.names=TRUE)
df_list <- map(list_filenames, read_csv)
df <- bind_rows(df_list)
```

# Read CollegeScorecardDatadictionary, id_name_link and MostRecentCohortsScorecardElements files.

```{r}
CollegeScorecardDatadictionary <- read_csv('../data/CollegeScorecardDatadictionary-09-08-2015.csv')
id_name_link <- read_csv('../data/id_name_link.csv')
Scorecard <- read_csv('../data/Most+Recent+Cohorts+(Scorecard+Elements).csv')
```

# As you can see below, we are just getting estimate scale from all 3 data sources. 
# Google trend dataset contains 1.5M records while scoredcard daatset contains ~7.8k records while id_name_link dataset contains ~3.6k records. This tell us that after inner joins + filtering certain conditions + aggregating google trends, our final dataframe would have less than ~3.6k unique college records.  

```{r}
nrow(df)
nrow(id_name_link)
nrow(Scorecard)
```

# Here, as a first step after data import, we will aggregate the Google Trends data. 
# Approach : Get the first ten characters from monthorweek, get the month from monthorweek and turn both results into date

```{r}
df <- df %>% mutate(date = str_sub(monthorweek, 1, 10)) %>% mutate(date_usable = ymd(date)) %>% select(-date) %>% mutate(month_of_record = floor_date(date_usable, 'month'))
```

# As the google trends data index is not an absolute index or using same calibration process across different google keywords/college names it maps to, we will need to normalize the data to have them at standardize scale. 
# To standardize index, we subtract mean and divide by standard deviation. 
# We also group by records to schoolname and month of the record so (schoolname,month) level summarization. 

```{r}
normalized_df <- df %>% drop_na(index) %>% group_by(schname,keyword) %>% mutate(index_standardized = (index - mean(index))/sd(index)) %>% group_by(schname, month_of_record) %>% summarize(mean_index_standardized = mean(index_standardized))
```

# PREDDEG column shows predominant degrees awarded by colleges so we filter only the schools that predominantly grant bachelor's degree. 

```{r}
Scorecard <- Scorecard %>% filter(PREDDEG == 3)
```

# To analyze earnings reported for alumni of these different colleges, we use "md_earn_wne_p10-REPORTED-EARNINGS" column from Scorecard dataset as that displays median earning reported after 10 years of enrollement for a particular college. 
# In Scorecard data, there are some colleges for which earnings column is either "PrivacySuppressed" or "NULL". As imported dataset column was "md_earn_wne_p10-REPORTED-EARNINGS" classified as char, we see "NULL" string instead of NA. 
# For both string terms "PrivacySuppressed" or "NULL", we can impute them with 0/mean value or we can just drop them as data isn't available. 
# I tried with 0 and mean value imputation but eventually, I decided to drop them as that was the best result for my executions. 

```{r}
sum(Scorecard$'md_earn_wne_p10-REPORTED-EARNINGS' == "PrivacySuppressed")
sum(Scorecard$'md_earn_wne_p10-REPORTED-EARNINGS' == "NULL") 
sum(is.na(Scorecard$'md_earn_wne_p10-REPORTED-EARNINGS'))
nrow(Scorecard)
Scorecard<-Scorecard[!(Scorecard$'md_earn_wne_p10-REPORTED-EARNINGS' == "NULL" | Scorecard$'md_earn_wne_p10-REPORTED-EARNINGS' == "PrivacySuppressed" ),]
nrow(Scorecard)
```

```{r}
##Scorecard$'md_earn_wne_p10-REPORTED-EARNINGS'[Scorecard$'md_earn_wne_p10-REPORTED-EARNINGS' == "PrivacySuppressed"] <- 0
##Scorecard$'md_earn_wne_p10-REPORTED-EARNINGS'[Scorecard$'md_earn_wne_p10-REPORTED-EARNINGS' == "NULL"] <- 0
```

# As I mentioned above, the default import classification of "md_earn_wne_p10-REPORTED-EARNINGS" is char, I created a new column by typecasting it to numeric data type. 

```{r}
Scorecard<-transform(Scorecard, earnings = as.numeric(Scorecard$'md_earn_wne_p10-REPORTED-EARNINGS'))
```

## In order to define high-earning and low-earning colleges, we calculate the Max, Min, Mean, 10th percentile, 25th percentile, 29th percentile 50th percentile, 79th percentile 90th percentile of the median earning of student working and not enrolled 10 years after entry.

## I also tried to defines some absolute values (low earning : below 25k and high earning : above 75k) to define thresholds, but evetually, quntile 29 and quantile 79 worked best. 


```{r}
max_earning <- max(Scorecard$earnings) 
min_earning <- min(Scorecard$earnings)
mean_earning <- mean(Scorecard$earnings)
quantile(Scorecard$earnings, probs = c(0.1, 0.25, 0.29, 0.75, 0.79, 0.9, 1))
earning_29 <- quantile(Scorecard$earnings, probs = c(0.29))
earning_79 <-quantile(Scorecard$earnings, probs = c(0.79))
```

# Drop those schools from id_name link data which are sharing the same name.

```{r}
length(unique(id_name_link$schname))
id_name_link <- id_name_link %>% group_by(schname) %>% mutate(n = n()) %>% filter(n==1) %>% select(-n)
nrow(id_name_link)

```

# Join google trends data to id_name_link by school name and then id_name_link table to Scorecard data by unitid. # Also, filter only rows where earnings are less than 29th percentile or greater than 79th percentile. 

```{r}
final_df <- normalized_df %>% inner_join(id_name_link, 'schname') %>% inner_join(Scorecard, by = c("unitid" = "UNITID")) %>% select(-c(OPEID, opeid6, INSTNM)) %>% filter(earnings >= earning_79 | earnings <= earning_29) %>% 
  mutate(earning_category = case_when(earnings >= earning_79 ~ '1', earnings <= earning_29 ~ '0')) %>%
  mutate(scorecard_implemented = case_when(month_of_record >= '2015-09-01' ~ '1', TRUE ~ '0'))
final_df <- final_df%>%mutate(part_of_the_year = case_when(month(month_of_record) <= 7 ~ '0', month(month_of_record) >7 ~ '1'))
final_df <- final_df%>%mutate(year_of_record = year(month_of_record))
final_df$year_info <- paste(final_df$part_of_the_year,"-",final_df$year_of_record)
```

# Trying to see how the data looks like in graph-Explore with month_of_record and mean_index_standardized

```{r}
ggplot(final_df, aes(x=month_of_record, y=mean_index_standardized,col=scorecard_implemented)) + geom_point() + geom_smooth(method = 'lm') + geom_vline(xintercept = as.Date("2015-09-01"))
m3 <- feols(mean_index_standardized~month_of_record, data = final_df)
```

# Split out those schools considered having high-earning and low-earning, group by month.

# In this assignment I consider high-earning as 79th percentile of earning and low-earning as 29th percentile of earning.

```{r}
high_earning_df <- final_df %>% filter(earnings >= earning_79)
low_earning_df <- final_df %>% filter(earnings <= earning_29)

length(unique(high_earning_df$schname))
length(unique(low_earning_df$schname))

```

# Run regression model and plot the month_of_record and mean of the standardized index to see the relationship

# From the regression result and graph for both high-earning dataframe and low-earning dataframe we can see there is negative relationship between the month and search interest. 

# As you can see in graph below, search trend can also be seen as a function of time. Regardless of scorecards implemented, every Jan-July month, search trend overall decreases for all high earning and all learning colleges on average and the jumps back in Aug and decreases again till December. This might be the correlation between when people apply for colleges or when term starts, students will check more information boosting search trend. 

```{r}
high_earning_df <- final_df %>% filter(earnings >= earning_79)
ggplot(high_earning_df, aes(x=month_of_record, y=mean_index_standardized, col=year_info)) + geom_smooth(method = 'lm',se=FALSE) + geom_vline(xintercept = as.Date("2015-09-01"))+ ggtitle("High-earning schools") + geom_point(aes(color = schname)) + theme(legend.position="none") 
m1 <- feols(mean_index_standardized~month_of_record, data = high_earning_df) 

low_earning_df <- final_df %>% filter(earnings <= earning_29)
ggplot(low_earning_df, aes(x=month_of_record, y=mean_index_standardized, col=year_info)) + geom_smooth(method = 'lm') + geom_vline(xintercept = as.Date("2015-09-01"))+ ggtitle("Low-earning schools") + geom_point(aes(color = schname)) + theme(legend.position="none")
m2 <- feols(mean_index_standardized~month_of_record, data = low_earning_df) 
etable(m1, m2)
```

#Filter and keep only earnings categorized as high and low earnings in the dataframe #Run regression model and plot the month_of_record and mean of the standardized index to see the relationship #From the regression result and graph we can see there is negative relationship between the month and search interest

```{r}
final_df_high_low <- final_df %>% filter(earnings >= earning_79 | earnings <= earning_29)
ggplot(final_df_high_low, aes(x=month_of_record, y=mean_index_standardized, col=scorecard_implemented)) + geom_point(aes(color = schname)) + theme(legend.position="none")  + geom_smooth(method = 'lm') + geom_vline(xintercept = as.Date("2015-09-01"))+ ggtitle("High/Low-earning schools")
m4 <- feols(mean_index_standardized~month_of_record, data = final_df_high_low)
etable(m3, m4)
```

#Plot high_earning_df and low_earning_df in same plot to see how data distributed #From the graph we can see that after Sep, 2015 there are some higher searching interests in high-earning schools(higher than before Sep, 2015), #and at the right bottom corner we can see one point is even lower than left bottom corner(Before Sep, 2015 is lower than after Sep, 2015)

```{r}
ggplot(NULL, aes(x=month_of_record, y=mean_index_standardized)) +    
  geom_point(data = high_earning_df, col = "red") +
  geom_point(data = low_earning_df, col = "blue") + 
  geom_vline(xintercept = as.Date("2015-09-01"))+ 
  ggtitle("Combined High/Low-earning Plot")
```

#To check total school numbers in each dataframe

```{r}
length(unique(final_df$schname))
length(unique(high_earning_df$schname))
length(unique(low_earning_df$schname))
```

#To look at the variable in final_df

```{r}
vtable(final_df)
```

#Run the code in the chunk below to look at the distribution of earnings:

```{r}
#ggplot(final_df, aes(x = earnings)) + geom_density()
```

#Forming the regression formula with Y(dependent variable) as mean of standardized index and X(independent variable) as log of earnings #First I want to see if [earning after graduating from school] have any relationship with search interest.
#Since the earnings has some extreme values, I decided to transform data by putting log on earnings to address partially right skewed data.
#From the regression result we can see that: A 10% change in earnings is associated with a 0.1\*0.0101 unit change in mean of standardized index.

```{r}
model1 <- feols(mean_index_standardized~log(earnings), data = final_df)
etable(model1)
ggplot(final_df, aes(x=log(earnings), y= mean_index_standardized)) + geom_point() + geom_smooth(method = 'lm')+ ggtitle("Log earnings")
```

#Forming the regression formula with Y(dependent variable) as mean of standardized index and X(independent variable) as whether implemented scorecard or not(binary variable) #From the regression result we can see that after implementing scorecard, the search interest actually decreased.

```{r}
model2 <- feols(mean_index_standardized~scorecard_implemented, data = final_df)
etable(model2)
ggplot(final_df, aes(x=scorecard_implemented, y= mean_index_standardized)) + geom_point() + geom_smooth(method = 'lm') + ggtitle("Scorecard_implemented")
```

#From the result from model2, I'm wondering after implementing scorecard, what is the effect of searching interest from high-earning/low-earning schools #Forming the regression formula with Y(dependent variable) as mean of standardized index and X(independent variable) as whether implemented scorecard or not(binary variable), adding the earning category(high/low earning) #From the regression result we can see that after implementing scorecard and with high-earning school, the search interest actually decreased.

```{r}
model3 <- feols(mean_index_standardized~scorecard_implemented + earning_category, data = final_df)
etable(model3)
ggplot(data=final_df, mapping = aes(x=scorecard_implemented, y= mean_index_standardized)) + geom_point(aes(color = earning_category)) + theme_bw() + ggtitle("Earning categories")
```

#From the result from model3, thinking there might have some potential variables is causing endogeneity.
Which makes the model result biased.
#Forming the regression formula with all the couple of variables which I think might cause endogeneity as control variable.
#From the regression result we can see that after implementing scorecard and with high-earning school and whether the school is under investigation, #whether the school is public/private for profit/private nonprofit, whether the school is closed or operating, the search interest actually decreased.
#From the result we can see that scorecard shifted interest between high and low-earning colleges in negative direction.
#For the coefficient of -0.3338 from scorecard_implement we can interpret as controlling all of other variables in the model, the difference in the search interests between implemented scorecard/not implemented scorecard.
#For the coefficient of 0.0091 from earning_category we can interpret as controlling all of other variables in the model, the difference in the search interests between high/low earning schools.
#For the research question we have for the project and based on the result of the regression model.
#We can see that the effect of implementing scorecard has different searching interest between high/low-earning schools by (0.068-0.3339+0.0091-0.0391*HCM2+0.0015*CONTROL-0.0038\*CURROPER)-unit, #though only coefficients of intercept and scorecard_implemented is statistically significant.

```{r}
model4 <- feols(mean_index_standardized~scorecard_implemented + earning_category + HCM2 + CONTROL + CURROPER, data = final_df)
etable(model4)
```

#Forming the regression formula with searching interest, scorecard and high/low-earning schools.
Assuming that the relationship between search interest is different for different values of high/low-earning schools.
#Putting Y as searching interests, X as scorecard, Z as high/low-earning schools and interaction term for XZ.
#From the regression result we can see that all coefficients of intercept, scorecard, high/low-earning and interaction term are all statistically significant, #which indicates that the result clearly reject the null hypothesis that all the coefficients are 0, all the coefficients of variables in the right hand side of model has effects on left hand side.
#We can use take derivative to get the effect of scorecard in the formula, in this model, the effect is (-0.287-0.0941*earning_category). #Means The effect of scorecard on search interest gets -0.3811 unit changes compare with high and low earning schools. #We can use take derivative to get the effect of earning_category in the formula, in this model, the effect is (0.0271-0.0941*scorecard_implemented).
#Means The effect of scorecard on search interest gets -0.067 unit changes compare with before and after implemented scorecard.

```{r}
model5 <- feols(mean_index_standardized~scorecard_implemented*earning_category, data = final_df)
etable(model5)
ggplot(final_df,aes(y=mean_index_standardized,x=scorecard_implemented,color=factor(earning_category)))+geom_point()+stat_smooth(method="lm",se=FALSE) + ggtitle("Interaction")
ggplot(final_df,aes(y=mean_index_standardized,x=scorecard_implemented,color=factor(earning_category)))+geom_point()+stat_smooth(method="lm",se=FALSE) + ggtitle("Interaction") + coord_cartesian(ylim = c(0,0.5))
ggplot(final_df,aes(y=mean_index_standardized,x=scorecard_implemented,color=factor(earning_category)))+geom_point()+stat_smooth(method="lm",se=FALSE) + ggtitle("Interaction") + coord_cartesian(ylim = c(0.5,1))
ggplot(final_df,aes(y=mean_index_standardized,x=scorecard_implemented,color=factor(earning_category)))+geom_point()+stat_smooth(method="lm",se=FALSE) + ggtitle("Interaction") + coord_cartesian(ylim = c(1,2))
ggplot(final_df,aes(y=mean_index_standardized,x=scorecard_implemented,color=factor(earning_category)))+geom_point()+stat_smooth(method="lm",se=FALSE) + ggtitle("Interaction") + coord_cartesian(ylim = c(2,3))
ggplot(final_df,aes(y=mean_index_standardized,x=scorecard_implemented,color=factor(earning_category)))+geom_point()+stat_smooth(method="lm",se=FALSE) + ggtitle("Interaction") + coord_cartesian(ylim = c(3,4))
ggplot(final_df,aes(y=mean_index_standardized,x=scorecard_implemented,color=factor(earning_category)))+geom_point()+stat_smooth(method="lm",se=FALSE) + ggtitle("Interaction") + coord_cartesian(ylim = c(4,5))
```

#To see all of regression results together

```{r}
etable(model1,model2,model3,model4,model5)
```

```{r}

```
